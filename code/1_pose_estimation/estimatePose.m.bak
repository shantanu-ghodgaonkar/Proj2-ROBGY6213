function [position, orientation] = estimatePose(data, t)
%% CHANGE THE NAME OF THE FUNCTION TO estimatePose
% Please note that the coordinates for each corner of each AprilTag are
% defined in the world frame, as per the information provided in the
% handout. Ideally a call to the function getCorner with ids of all the
% detected AprilTags should be made. This function should return the X and
% Y coordinate of each corner, or each corner and the centre, of all the
% detected AprilTags in the image. You can implement that anyway you want
% as long as the correct output is received. A call to that function
% should made from this function.
    %% Input Parameter Defination
    % data = the entire data loaded in the current dataset
    % t = index of the current data in the dataset
    
    %% Output Parameter Defination
    
    % position = translation vector representing the position of the
    % drone(body) in the world frame in the current time, in the order ZYX
    
    % orientation = euler angles representing the orientation of the
    % drone(body) in the world frame in the current time, in the order ZYX

%% MY IMPLEMENTATION START ------------------------------------------------
% tic

% Camera intrinsic matrix
k = [311.0520, 0, 201.8724; 0, 311.3885, 113.6210; 0, 0, 1]; 
% k = [3.110520, 0, 2.018724; 0, 3.113885, 1.136210; 0, 0, 1]; 

% Quaternion matrix
q = zeros(4, length(data(t).id));

% Number of detected corners weight matrix 
w = 4 * eye(length(data(t).id)); 

% Translation vector
T = zeros(3, 1); 

% Iterate over all the detected April Tags
for i = 1:length(data(t).id)
    % Get corners of each April Tag in the world frame
    pw = getCorner(data(t).id(i));
    % Get corners of each April Tag in the camera frame
    pc = [data(t).p1(:,i), data(t).p2(:,i), data(t).p3(:,i), data(t).p4(:,i)];
  
    % Find the A matrix 
    A = [   pw(1,1), pw(2,1), 1, 0, 0, 0, (-pc(1,1)*pw(1,1)), (-pc(1,1)*pw(2,1)), (-pc(1,1));
        0, 0, 0, pw(1,1), pw(2,1), 1, (-pc(2,1)*pw(1,1)), (-pc(2,1)*pw(2,1)), (-pc(2,1));
        pw(1,2), pw(2,2), 1, 0, 0, 0, (-pc(1,2)*pw(1,2)), (-pc(1,2)*pw(2,2)), (-pc(1,2));
        0, 0, 0, pw(1,2), pw(2,2), 1, (-pc(2,2)*pw(1,2)), (-pc(2,2)*pw(2,2)), (-pc(2,2));
        pw(1,3), pw(2,3), 1, 0, 0, 0, (-pc(1,3)*pw(1,3)), (-pc(1,3)*pw(2,3)), (-pc(1,3));
        0, 0, 0, pw(1,3), pw(2,3), 1, (-pc(2,3)*pw(1,3)), (-pc(2,3)*pw(2,3)), (-pc(2,3));
        pw(1,4), pw(2,4), 1, 0, 0, 0, (-pc(1,4)*pw(1,4)), (-pc(1,4)*pw(2,4)), (-pc(1,4));
        0, 0, 0, pw(1,4), pw(2,4), 1, (-pc(2,4)*pw(1,4)), (-pc(2,4)*pw(2,4)), (-pc(2,4));];

    % Perform SVD of A to get the V matrix 
    [~, ~, V] = svd(A);
   
    % Extract 9th column of V and scale it using the sign of 
    % the last element of the last column of V
    hl = [V(1:3,9), V(4:6,9), V(7:9,9)]';
    % [~,sigma,~] = svd(hl);
    h = hl / V(9,9);

    % Find RT = [R1 R2 T]
    RT = inv(k) * h;
    % Extract R1
    R1 = RT(:,1);
    R1_skew = [0, -R1(3,1), R1(2,1); R1(3,1), 0, -R1(1,1); -R1(2,1), R1(1,1), 0];
    % Extract R2
    R2 = RT(:,2);
    % Extract T
    T_temp = RT(:,3);
    
    % Compute the SVD of the newly formed orthogonal matrix
    [Ur, ~, Vr] = svd([R1, R2, R1_skew*R2]);

    % Compute the rotation matrix from the orthogonal matrix
    R = Ur * [1, 0, 0; 0, 1, 0; 0, 0, det(Ur*Vr)] * Vr;
    
    % Sclae T
    T_temp = T_temp / norm(R1) ;
    
    % Add all weighted T vectors for averaging later
    T = T + (4 * T_temp);
    
    % Find the quaternion based on the rotation matrix and concatenate it 
    % to the quaternion matrix
    q(:,i) = rotm2quat(R);
end

% Compute the s matrix, as is given in the paper "Quaternion Attitude 
% Estimation Using Vector Observations" by F. Landis Markley and 
% Daniele Mortari
s = q * w * q';

% Find the eigenvector corresponding to the largest eigenvalue to get the
% averaged quaternion
[V, ~] = eigs(s, 1,'lm');

% Convert the averaged quaternion to a rotation matrix
R = quat2rotm(V');

% Average the summed up Translation Vectors
T = T /(4 * length(data(t).id));

% Combine the rotational and translational parts
HT = [[R, T] ; [0,0,0,1]];

% Find the Homogenous transform to go from camera frame to imu frame
c2b = [[eul2rotm([-pi/4,0,0]), [-0.04, 0.0, -0.03]']; [0,0,0,1]];

% Transform from camera frame to world frame
HT = c2b * HT;
% disp("DEBUG POINT");

% Set the position
position = HT(1:3, 4);

% Set the orientation
orientation = rotm2eul(HT(1:3, 1:3)) ;

% toc
%% MY IMPLEMENTATION END --------------------------------------------------
end